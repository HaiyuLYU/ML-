# 西瓜书复习笔记08
- 个体与集成：
    - 集成学习的结构：
        先产生一组个体学习器，再使用某种策略将它们结合起来。
    - 个体学习器：
        - 同质：
            同质集成中，个体学习器算法都是统一的，也叫做基学习器、弱学习器。
        - 异质：
            异质集成中，个体学习器可能由不同算法生成，例如同时包括决策树和神经网络。
    - 同质学习器的两类：
        Boosting：弱学习器之间有强依赖关系，必须通过串行生成的序列化方法。
        Bagging：弱学习器之间没有依赖关系，用同时生成的并行化方法。
— Boosting:
    - 什么是Boosting：
        Boosting是一种将弱学习器提升为强学习器的算法。先训练一个弱学习器；再根据弱学习器的表现对训练样本进行调整，使得先前做错的训练样本在后续收到更多关注；然后基于调整后的样本继续训练下一个弱学习器。最后弱学习器数目到达定值，将这几个弱学习器进行加权结合。
    - 弱学习器没达到特定数目怎么办：
        一旦不满足条件（例如弱学习器的误差率>0.5）,那么将这个学习器抛弃。这时还没到预定的学习轮数。我们需要对数据进行重采样，根据当前的样本分布重新对训练样本进行采样，再基于新采样的结果训练弱学习器。这样可以防止早停。
    - Boosting方法主要关注降低偏差。
    - AdaBoost:
        - 用于分类的算法，仅限于二分类[-1,1]
        - 思想：
            前一个弱分类器分错的样本会得到加强，加权后的全体样本再次用来训练下一个弱分类器。
        - 步骤：
            1. 初始样本的权值分布\(\frac{1}{N}\)：
                $$ D_{1}=\left(W_{11}, w_{12} \cdots W_{1 N}\right) \quad W_{1 i}=\frac{1}{N} $$
            2. 多轮迭代：
                如果样本被正确分类，权值降低；
                如果被错误分类，权值变高
                计算本轮学习器权重：
                    $$ \alpha_{t}=\frac{1}{2} \ln \left(\frac{1-\epsilon_{t}}{\epsilon_{t}}\right) $$
                其中：
                \(\epsilon_{t}\)为误差率
                计算下一轮样本权重：
                    $$w_{m+1, i}=\frac{w_{m i}}{z_{m}} \exp \left(-a_{m} y_{i} \operatorname{G_m}\left(x_{i}\right)\right) $$
                其中：
                \(G_m\)为弱分类器
                \(Z_m\)是归一化因子
            3. 组合弱学习器：
                sign为阶跃函数：
                    $$ sign \left\{\begin{array}{l}1, x>0 \\ 0, x=0 \\ -1, x<0\end{array}\right. $$
                $$ G(x) = sign\left(\sum_{m=1}^{m} a_{m} G_{m}(x)\right) $$
        - 损失函数：
                $$ loss(x) = \exp \left(\hat{y}_{i} f\left(x_{i}\right)\right) $$
            求导：
                $$ -e^{f(x)} p(f(x)=1 | x)+e^{f(x)} p(f(x)=-1 | x) = 0 $$
            求解：
                $$ f(x)=\frac{1}{2} \ln \left(\frac{p(f(x) = 1 | x)}{p(f(x)= -1 | x)}\right) $$
        - 何时停止：
            到达预定的错误率
            到达预定的迭代数
        - 在SkLearn里，AdaBoost默认是树模型。