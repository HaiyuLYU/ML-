# 西瓜书复习笔记08
- 个体与集成：
    - 集成学习的结构：
        先产生一组个体学习器，再使用某种策略将它们结合起来。
    - 个体学习器：
        - 同质：
            同质集成中，个体学习器算法都是统一的，也叫做基学习器、弱学习器。
        - 异质：
            异质集成中，个体学习器可能由不同算法生成，例如同时包括决策树和神经网络。
    - 同质学习器的两类：
        Boosting：弱学习器之间有强依赖关系，必须通过串行生成的序列化方法。
        Bagging：弱学习器之间没有依赖关系，用同时生成的并行化方法。
— Boosting:
    - 什么是Boosting：
        Boosting是一种将弱学习器提升为强学习器的算法。先训练一个弱学习器；再根据弱学习器的表现对训练样本进行调整，使得先前做错的训练样本在后续收到更多关注；然后基于调整后的样本继续训练下一个弱学习器。最后弱学习器数目到达定值，将这几个弱学习器进行加权结合。
    - 弱学习器没达到特定数目怎么办：
        一旦不满足条件（例如弱学习器的误差率>0.5）,那么将这个学习器抛弃。这时还没到预定的学习轮数。我们需要对数据进行重采样，根据当前的样本分布重新对训练样本进行采样，再基于新采样的结果训练弱学习器。这样可以防止早停。
    - Boosting方法主要关注降低偏差。
    - AdaBoost:
        - 用于分类的算法，仅限于二分类[-1,1]
        - 思想：
            前一个弱分类器分错的样本会得到加强，加权后的全体样本再次用来训练下一个弱分类器。
        - 步骤：
            1. 初始样本的权值分布\(\frac{1}{N}\)：
                $$ D_{1}=\left(W_{11}, w_{12} \cdots W_{1 N}\right) \quad W_{1 i}=\frac{1}{N} $$
            2. 多轮迭代：
                如果样本被正确分类，样本权值降低；
                如果被错误分类，样本权值变高
                计算本轮学习器权重：
                    $$ \alpha_{t}=\frac{1}{2} \ln \left(\frac{1-\epsilon_{t}}{\epsilon_{t}}\right) $$
                其中：
                \(\epsilon_{t}\)为误差率
                计算下一轮样本权重：
                    $$w_{m+1, i}=\frac{w_{m i}}{z_{m}} \exp \left(-a_{m} y_{i} \operatorname{G_m}\left(x_{i}\right)\right) $$
                其中：
                \(G_m\)为弱分类器
                \(Z_m\)是归一化因子
            3. 组合弱学习器：
                加大分类错误率小的学习器的权重；
                减小分类错误率大的学习器的权重。
                sign为阶跃函数：
                    $$ sign \left\{\begin{array}{l}1, x>0 \\ 0, x=0 \\ -1, x<0\end{array}\right. $$
                $$ G(x) = sign\left(\sum_{m=1}^{m} a_{m} G_{m}(x)\right) $$
        - 损失函数：
                $$ loss(x) = \exp \left(\hat{y}_{i} f\left(x_{i}\right)\right) $$
            求导：
                $$ -e^{f(x)} p(f(x)=1 | x)+e^{f(x)} p(f(x)=-1 | x) = 0 $$
            求解：
                $$ f(x)=\frac{1}{2} \ln \left(\frac{p(f(x) = 1 | x)}{p(f(x)= -1 | x)}\right) $$
        - 何时停止：
            到达预定的错误率
            到达预定的迭代数
        - 在SkLearn里，AdaBoost默认是树模型。
    - GBDT（梯度提升树）：
        - 用于回归的算法，虽然经过调整后可以进行分类，但是GBDT是回归树。
        - 思想：
            GBDT核心在于每一棵树拟合前面树的残差；在残差减小的方向（负梯度）建立一个新模型；最后累加所有树的结果作为最终的结果。显然分类树结果是无法累加的。
        - 分类树与回归树在处理连续值时的区别：
            分类树是穷举每个特征的每个阈值，按最大信息增益或者最小基尼指数来分裂节点。
            回归树是也是穷举每个特征的阈值，按组小均方误差来确定分裂点。
        - 如果训练集不变，那么训练三次得到的树是一样的。
        - GBDT的结合策略不是投票法也不是平均法，而是累加所有树的结果作为最终结果。
        - 过程：
            GBDT使用多轮迭代,每轮迭代产生一个弱学习器，每个学习器器是在上一轮学习结果的残差基础上进行训练，最后将所有学习器的结果累加。
        - 哪里体现了梯度：
            如果损失函数为均方误差的情况下，要保证每一轮保证损失函数最小。对损失函数求导得到\((y^* - y)\)刚好是残差，也就是负梯度就是残差。
            但是如果损失函数不是均方误差，比如最大熵损失，那么负梯度就不是残差了。所以说提升树都是基于梯度的，只不过均方误差求导后刚好是残差。
        - 停止条件：
            - 直到每个叶子上的年龄都唯一（很难），一般是用叶子上的平均值作为该节点的预测值。
            - 到达规定的叶子上限。
        - sklearn中主要参数：
            - 每回合树的深度
            - 学习器个数
    - XgBoost：
        作者陈天奇，华盛顿大学计算机系毕业，华人之光！
        - 目标函数：
        - 树的定义：
        - 树的复杂度：
        - 叶子结点归组：
        - 树结构打分：
        - 节点分裂：
            使用贪心算法：
            从深度为0开始，遍历每个候选结点的每个特征；
            对每个特征按特征值大小排序；
            线性扫描，找出每个特征对应的最佳特征值；
            最后在所有特征中找出最好的分裂点（分裂后增益Gain最大的特征）
        - 如何加快寻找最佳分裂结点：
            并行查找： XGBoost利用多个线程并行计算每个特征的最佳分裂点。
        - 分裂停止条件：
            Gain<设定的阈值
            树到达最大深度
            样本权重小于设定值

    - LightGBM：